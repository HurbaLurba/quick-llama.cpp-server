# Mistral Small 3.2 24B Vision Container using pre-compiled llama.cpp base
FROM llama-base:latest

# Mistral Small 3.2 24B Model Configuration
ENV MODEL_REPO="unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF"
ENV MODEL_FILE="Mistral-Small-3.2-24B-Instruct-2506-IQ4_NL.gguf"
ENV MODEL_QUANT="IQ4_NL"
ENV MMPROJ_REPO="unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF"
ENV MMPROJ_FILE="mmproj-F32.gguf"
ENV CONTEXT_SIZE="131072"
ENV MODEL_ALIAS="mistral-small-3.2-24b-vision"

# Mistral Reasoning Configuration (full reasoning support enabled)
ENV REASONING_FORMAT="deepseek"
ENV REASONING_BUDGET="-1"
ENV THINKING_FORCED_OPEN="false"
ENV CHAT_TEMPLATE="mistral"

# 32GB VRAM ADA/BLACKWELL Performance Optimization (stable settings for quality)
ENV BATCH_SIZE="2048"
ENV UBATCH_SIZE="2048"
ENV PARALLEL_SEQUENCES="1"
ENV FLASH_ATTENTION="on"
ENV N_GPU_LAYERS="-1"
ENV CACHE_TYPE_K="f16"
ENV CACHE_TYPE_V="f16"
ENV NO_MMAP="false"
ENV MAX_TOKENS="16384"
ENV CPU_MOE="false"

# Copy startup script
COPY scripts/ /workspace/scripts/

# Set permissions and fix line endings
RUN chmod +x /workspace/scripts/*.sh && \
    find /workspace/scripts -name "*.sh" -exec sed -i 's/\r$//' {} \;

# Set working directory
WORKDIR /workspace

# Expose port
EXPOSE 8080

# Start the server
ENTRYPOINT ["/workspace/scripts/start-mistral-small-3.2-24b-vision.sh"]
