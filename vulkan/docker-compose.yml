services:
  # Custom LLaMA Server - Mistral Small 3.2 24B Vision with Vulkan support
  # Enhanced device access based on Toxantron/iGPU-Docker methodology
  mistral-small-3.2-24b-vision-vulkan:
    image: custom-llama-cpp-server-vulkan:latest
    build:
      context: .
      dockerfile: Dockerfile.custom-llama-cpp-server
    container_name: mistral-small-3.2-24b-vision-vulkan
    environment:
      - MODEL_SCRIPT=mistral-small-3.2-24b-vision-vulkan
      # AMD-focused Vulkan environment variables
      - VK_LAYER_PATH=/usr/share/vulkan/explicit_layer.d
      - VK_ICD_FILENAMES=/usr/share/vulkan/icd.d/radeon_icd.x86_64.json:/usr/share/vulkan/icd.d/intel_icd.x86_64.json:/usr/share/vulkan/icd.d/nvidia_icd.json
      - VULKAN_SDK=/usr
      - VK_LOADER_DEBUG=warn
      # VK_INSTANCE_LAYERS explicitly NOT set - causes empty layer errors
      # AMD-specific optimizations for Ryzen AI and APU graphics
      - AMD_VULKAN_ICD=RADV
      - RADV_PERFTEST=aco,llvm
      - MESA_VK_VERSION_OVERRIDE=1.3
      - MESA_LOADER_DRIVER_OVERRIDE=radeonsi
      # Force Vulkan device selection and enable debugging
      - GGML_VULKAN_DEVICE=0
      - GGML_VULKAN_CHECK_RESULTS=1
      # AMD iGPU specific (8945HS has gfx1103 architecture like 780M)
      - HSA_OVERRIDE_GFX_VERSION=11.0.2
    ports:
      - "8085:8080"
    volumes:
      - ${USERPROFILE:-~}/.cache/llama:/root/.cache/llama
    # Enhanced device access following Toxantron methodology
    devices:
      - /dev/dri:/dev/dri
    # Group permissions - use same pattern as ROCm setup
    group_add:
      - "${RENDER_GROUP_ID:-102}"
      - "${VIDEO_GROUP_ID:-44}"
    # Remove privileged mode - not needed with proper group setup
    restart: unless-stopped
    # Network mode for better WSL2 compatibility
    network_mode: "bridge"
