services:
  # Gemma 3 27B IT Abliterated Vision with ROCm (Windows - CPU only)
  gemma3-27b-it-abliterated-vision-rocm-windows:
    image: ghcr.io/ggml-org/llama.cpp:server-rocm
    container_name: gemma3-27b-it-abliterated-vision-rocm-windows
    ports:
      - "8085:8080"
    # No device access for Windows Docker Desktop
    # devices:
    #   - /dev/kfd:/dev/kfd  # Not available in Windows Docker
    #   - /dev/dri:/dev/dri  # Not available in Windows Docker
    
    # ROCm environment variables for AMD 8945HS (gfx1103) - software mode
    environment:
      - HSA_OVERRIDE_GFX_VERSION=11.0.2
      - HCC_AMDGPU_TARGET=gfx1103
      - ROCM_PATH=/opt/rocm
      - HIP_PLATFORM=amd
      - HSA_ENABLE_SDMA=0
      - ROCR_VISIBLE_DEVICES=0
      - HIP_VISIBLE_DEVICES=0
    # Model cache volume (separate for Windows)
    volumes:
      - llama_cache_windows:/root/.cache/huggingface
    # Start with Gemma 3 model (CPU-optimized settings)
    command: [
      "--hf-repo", "mlabonne/gemma-3-27b-it-abliterated-GGUF",
      "--hf-file", "gemma-3-27b-it-abliterated-Q4_K_M.gguf",
      "--mmproj", "/root/.cache/huggingface/mlabonne--gemma-3-27b-it-abliterated-GGUF/mmproj-mlabonne_gemma-3-27b-it-abliterated-f16.gguf",
      "--host", "0.0.0.0",
      "--port", "8080",
      "-ngl", "0",
      "-c", "8192",
      "--alias", "gemma3-27b-it-abliterated-rocm-windows"
    ]
    restart: unless-stopped

  # Mistral Small 3.2 24B Vision with ROCm (Windows - CPU only)
  mistral-small-3.2-24b-vision-rocm-windows:
    image: ghcr.io/ggml-org/llama.cpp:server-rocm
    container_name: mistral-small-3.2-24b-vision-rocm-windows
    ports:
      - "8085:8080"
    # No device access for Windows Docker Desktop
    # devices:
    #   - /dev/kfd:/dev/kfd  # Not available in Windows Docker
    #   - /dev/dri:/dev/dri  # Not available in Windows Docker
    
    # ROCm environment variables for AMD 8945HS (gfx1103) - software mode
    environment:
      - HSA_OVERRIDE_GFX_VERSION=11.0.2
      - HCC_AMDGPU_TARGET=gfx1103
      - ROCM_PATH=/opt/rocm
      - HIP_PLATFORM=amd
      - HSA_ENABLE_SDMA=0
      - ROCR_VISIBLE_DEVICES=0
      - HIP_VISIBLE_DEVICES=0
    # Model cache volume (separate for Windows)
    volumes:
      - llama_cache_windows:/root/.cache/huggingface
    # Start with Mistral Small model (CPU-optimized settings)
    command: [
      "--hf-repo", "unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF",
      "--hf-file", "Mistral-Small-3.2-24B-Instruct-2506-UD-Q4_K_XL.gguf",
      "--mmproj", "/root/.cache/huggingface/unsloth--Mistral-Small-3.2-24B-Instruct-2506-GGUF/mmproj-F32.gguf",
      "--host", "0.0.0.0",
      "--port", "8080",
      "-ngl", "0",
      "-c", "8192",
      "--reasoning-format", "deepseek",
      "--alias", "mistral-small-3.2-24b-vision-rocm-windows"
    ]
    restart: unless-stopped

volumes:
  llama_cache_windows:
